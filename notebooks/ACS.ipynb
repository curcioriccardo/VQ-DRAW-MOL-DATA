{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/curcioriccardo/VQ-DRAW-MOL-DATA/blob/main/notebooks/ACS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odIwH3VfC59Y"
      },
      "source": [
        "## Imports and Mount\n",
        "\n",
        "Here you can choose where to store the model by changing path variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ7Cl9tzTCl-",
        "outputId": "3d56f86f-620b-4860-bb9e-a04bf883d406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import gzip\n",
        "import pandas\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import h5py\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn import model_selection\n",
        "import shutil\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "\n",
        "PATH = F\"/content/gdrive/MyDrive/Smiles_models/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO0Mx0OWDEhK"
      },
      "source": [
        "## Utility functions\n",
        "\n",
        "These are utility functions to load dataset and decode tensors and tokenize SMILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aXdqRqQTLEX"
      },
      "outputs": [],
      "source": [
        "def one_hot_array(i, n):\n",
        "    return map(int, [ix == i for ix in xrange(n)])\n",
        "\n",
        "def one_hot_index(vec, charset):\n",
        "    return map(charset.index, vec)\n",
        "\n",
        "def from_one_hot_array(vec):\n",
        "    oh = np.where(vec == 1)\n",
        "    if oh[0].shape == (0, ):\n",
        "        return None\n",
        "    return int(oh[0][0])\n",
        "\n",
        "def decode_smiles_from_indexes(vec, charset):\n",
        "    smiles=\"\"\n",
        "    charset = list(map(lambda x: x.decode('utf-8'), charset))\n",
        "    vec = vec.tolist()\n",
        "    for row in vec:\n",
        "      smiles+= str(charset[np.argmax(row)])\n",
        "\n",
        "    return smiles.strip()\n",
        "\n",
        "def load_dataset(filename, split = True):\n",
        "    h5f = h5py.File(filename, 'r')\n",
        "    if split:\n",
        "        data_train = h5f['data_train'][:]\n",
        "    else:\n",
        "        data_train = None\n",
        "    data_test = h5f['data_test'][:]\n",
        "    charset =  h5f['charset'][:]\n",
        "    h5f.close()\n",
        "    if split:\n",
        "        return (data_train, data_test, charset)\n",
        "    else:\n",
        "        return (data_test, charset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6JUhymHDYmW"
      },
      "source": [
        "## Model\n",
        "\n",
        "This is a recreation of the VAE model architecture used in https://pubs.acs.org/doi/10.1021/acscentsci.7b00572\n",
        "\n",
        "The encoder uses\n",
        "three 1D convolutional layers followed by one fully\n",
        "connected. The decoder feeds into three\n",
        "layers of gated recurrent unit (GRU)\n",
        "The last layer of the RNN decoder defines a probability\n",
        "distribution over all possible characters at each position in the\n",
        "SMILES string. This means that the writeout operation is\n",
        "stochastic, and the same point in latent space may decode into\n",
        "different SMILES strings, depending on the random seed used\n",
        "to sample characters. #The output GRU layer had one additional\n",
        "input, corresponding to the character sampled from the softmax\n",
        "output of the previous time step and was trained using teacher\n",
        "forcing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGAirp3UUDmt"
      },
      "outputs": [],
      "source": [
        "class MolecularVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MolecularVAE, self).__init__()\n",
        "\n",
        "        self.conv_1 = nn.Conv1d(120, 9, kernel_size=9)\n",
        "        self.conv_2 = nn.Conv1d(9, 9, kernel_size=9)\n",
        "        self.conv_3 = nn.Conv1d(9, 10, kernel_size=11)\n",
        "        self.linear_0 = nn.Linear(70, 435)\n",
        "        self.linear_1 = nn.Linear(435, 292)\n",
        "        self.linear_2 = nn.Linear(435, 292)\n",
        "\n",
        "        self.linear_3 = nn.Linear(292, 292)\n",
        "        self.gru = nn.GRU(292, 501, 3, batch_first=True)\n",
        "        self.linear_4 = nn.Linear(501, 33)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.relu(self.conv_1(x))\n",
        "        x = self.relu(self.conv_2(x))\n",
        "        x = self.relu(self.conv_3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.selu(self.linear_0(x))\n",
        "        return self.linear_1(x), self.linear_2(x)\n",
        "\n",
        "    def sampling(self, z_mean, z_logvar):\n",
        "        epsilon = 1e-2 * torch.randn_like(z_logvar)\n",
        "        return torch.exp(0.5 * z_logvar) * epsilon + z_mean\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = F.selu(self.linear_3(z))\n",
        "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, 120, 1)\n",
        "        output, hn = self.gru(z)\n",
        "        out_reshape = output.contiguous().view(-1, output.size(-1))\n",
        "        y0 = F.softmax(self.linear_4(out_reshape), dim=1)\n",
        "        y = y0.contiguous().view(output.size(0), -1, y0.size(-1))\n",
        "        return y\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_mean, z_logvar = self.encode(x)\n",
        "        z = self.sampling(z_mean, z_logvar)\n",
        "        return self.decode(z), z_mean, z_logvar\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq12cN0gDik7"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Here we will download the same dataset used for VQ-DRAW and MRL.\n",
        "For ease of computation, the strings are encoded up to a\n",
        "maximum length of 120 characters.Only canonicalized SMILES are used for training to\n",
        "avoid dealing with equivalent SMILES representations. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psCpULS_AQMg",
        "outputId": "e00e6d18-9dae-405e-c5eb-7e7d9b3b854f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'molecular-vae': No such file or directory\n",
            "Cloning into 'molecular-vae'...\n",
            "remote: Enumerating objects: 188, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 188 (delta 0), reused 0 (delta 0), pack-reused 185\u001b[K\n",
            "Receiving objects: 100% (188/188), 2.99 MiB | 14.56 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -R 'molecular-vae'\n",
        "!git clone https://github.com/aksub99/molecular-vae.git\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/molecular-vae/data/processed.zip', 'r')\n",
        "zip_ref.extractall('molecular-vae/data/')\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3ILWmXoDvRK"
      },
      "source": [
        "## Loss\n",
        "\n",
        "Minimizes Binary Cross Entropy and a KL term with a 0.5 weigth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y3wDuFyEHf7"
      },
      "outputs": [],
      "source": [
        "def vae_loss(x_decoded_mean, x, z_mean, z_logvar):\n",
        "    xent_loss = F.binary_cross_entropy(x_decoded_mean, x, size_average=False)\n",
        "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
        "    return xent_loss + kl_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TDOGiqtEJbe"
      },
      "source": [
        "## Training\n",
        "\n",
        "This is the training code. You can change the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "5r0gV2l-AtAS",
        "outputId": "b7fc46f7-cac5-4e13-90c4-3cdf87c8131d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 120, 33])\n",
            "torch.float32\n",
            "train= 0.0  test= tensor(535.0306, device='cuda:0')\n",
            "torch.Size([64, 120, 33])\n",
            "torch.float32\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a1d66fa58137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-a1d66fa58137>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "data_train, data_test, charset = load_dataset('/content/molecular-vae/data/processed.h5')\n",
        "data_train = torch.utils.data.TensorDataset(torch.from_numpy(data_train))\n",
        "data_test = torch.utils.data.TensorDataset(torch.from_numpy(data_test))\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=64, shuffle=True)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "epochs = 100\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = MolecularVAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    # Training phase\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "\n",
        "        data = data[0].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, mean, logvar = model(data)\n",
        "        \n",
        "        if batch_idx==0:\n",
        "              inp = data.cpu().numpy()\n",
        "              outp = output.cpu().detach().numpy()\n",
        "              lab = data.cpu().numpy()\n",
        "              print(\"Input:\")\n",
        "              print(decode_smiles_from_indexes(inp[0], charset))\n",
        "              print(\"Label:\")\n",
        "              print(decode_smiles_from_indexes(lab[0], charset))\n",
        "              sampled= outp[0]\n",
        "              print(\"Output:\")\n",
        "              print(decode_smiles_from_indexes(sampled, charset))\n",
        "        \n",
        "        loss = vae_loss(output, data, mean, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss\n",
        "        optimizer.step()\n",
        "        data.detach()\n",
        "    # Validation phase\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "     for batch_idx, data in enumerate(test_loader):\n",
        "        data = data[0].to(device)\n",
        "        output, mean, logvar = model(data)\n",
        "        loss = vae_loss(output, data, mean, logvar)\n",
        "        test_loss += loss\n",
        "        data.detach()\n",
        "\n",
        "    \n",
        "\n",
        "    print('train=', (train_loss / len(train_loader.dataset)), ' test=',(test_loss / len(test_loader.dataset)))\n",
        "     \n",
        "    return\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "\n",
        "torch.save(model,PATH+\"ACS_epoch_\"+str(epochs)+\".pth\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuPZnm-aYPLu"
      },
      "outputs": [],
      "source": [
        "# 1,33 minutes per epoch"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}